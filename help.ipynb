{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from enum import Enum\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum class representing different embedding providers\n",
    "class EmbeddingProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    COHERE = \"cohere\"\n",
    "    AMAZON_BEDROCK = \"bedrock\"\n",
    "\n",
    "# Enum class representing different model providers\n",
    "class ModelProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    GROQ = \"groq\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON_BEDROCK = \"bedrock\"\n",
    "\n",
    "\n",
    "def get_langchain_embedding_provider(provider: EmbeddingProvider, model_id: str = None):\n",
    "    \"\"\"\n",
    "    Returns an embedding provider based on the specified provider and model ID.\n",
    "\n",
    "    Args:\n",
    "        provider (EmbeddingProvider): The embedding provider to use.\n",
    "        model_id (str): Optional -  The specific embeddings model ID to use .\n",
    "\n",
    "    Returns:\n",
    "        A LangChain embedding provider instance.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified provider is not supported.\n",
    "    \"\"\"\n",
    "    if provider == EmbeddingProvider.OPENAI:\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings()\n",
    "    elif provider == EmbeddingProvider.COHERE:\n",
    "        from langchain_cohere import CohereEmbeddings\n",
    "        return CohereEmbeddings()\n",
    "    elif provider == EmbeddingProvider.AMAZON_BEDROCK:\n",
    "        from langchain_community.embeddings import BedrockEmbeddings\n",
    "        return BedrockEmbeddings(model_id=model_id) if model_id else BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding provider: {provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = get_langchain_embedding_provider(EmbeddingProvider.OPENAI)\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/saikr/Downloads/kickstartai/Github/chatbot/Shivam Rai - Resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector_store = encode_pdf(path, chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "retriever = chunks_vector_store.as_retriever()\n",
    "\n",
    "# Set up system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "    \n",
    "])\n",
    "\n",
    "# Create the question-answer chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main skillset of the candidate includes regressions, KNN, decision trees, random forests, and boosting techniques such as GBM and XGB. They are experienced in all aspects of projects, including analytics to leverage skills in understanding market opportunities and patient behavior.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer= rag_chain.invoke({\"input\": \"What is the main skillset of the candidate?\"})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
